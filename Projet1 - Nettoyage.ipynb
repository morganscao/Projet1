{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcours Data scientist - Analysez des données nutritionnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "(320772, 162)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>url</th>\n",
       "      <th>creator</th>\n",
       "      <th>created_t</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>last_modified_t</th>\n",
       "      <th>last_modified_datetime</th>\n",
       "      <th>product_name</th>\n",
       "      <th>generic_name</th>\n",
       "      <th>quantity</th>\n",
       "      <th>...</th>\n",
       "      <th>ph_100g</th>\n",
       "      <th>fruits-vegetables-nuts_100g</th>\n",
       "      <th>collagen-meat-protein-ratio_100g</th>\n",
       "      <th>cocoa_100g</th>\n",
       "      <th>chlorophyl_100g</th>\n",
       "      <th>carbon-footprint_100g</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition-score-uk_100g</th>\n",
       "      <th>glycemic-index_100g</th>\n",
       "      <th>water-hardness_100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000003087</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>openfoodfacts-contributors</td>\n",
       "      <td>1474103866</td>\n",
       "      <td>2016-09-17T09:17:46Z</td>\n",
       "      <td>1474103893</td>\n",
       "      <td>2016-09-17T09:18:13Z</td>\n",
       "      <td>Farine de blé noir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1kg</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000000004530</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>Banana Chips Sweetened (Whole)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000000004559</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>Peanuts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000000016087</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489055731</td>\n",
       "      <td>2017-03-09T10:35:31Z</td>\n",
       "      <td>1489055731</td>\n",
       "      <td>2017-03-09T10:35:31Z</td>\n",
       "      <td>Organic Salted Nut Mix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000000016094</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489055653</td>\n",
       "      <td>2017-03-09T10:34:13Z</td>\n",
       "      <td>1489055653</td>\n",
       "      <td>2017-03-09T10:34:13Z</td>\n",
       "      <td>Organic Polenta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 162 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            code                                                url  \\\n",
       "0  0000000003087  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "1  0000000004530  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "2  0000000004559  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "3  0000000016087  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "4  0000000016094  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "\n",
       "                      creator   created_t      created_datetime  \\\n",
       "0  openfoodfacts-contributors  1474103866  2016-09-17T09:17:46Z   \n",
       "1             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
       "2             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
       "3             usda-ndb-import  1489055731  2017-03-09T10:35:31Z   \n",
       "4             usda-ndb-import  1489055653  2017-03-09T10:34:13Z   \n",
       "\n",
       "  last_modified_t last_modified_datetime                    product_name  \\\n",
       "0      1474103893   2016-09-17T09:18:13Z              Farine de blé noir   \n",
       "1      1489069957   2017-03-09T14:32:37Z  Banana Chips Sweetened (Whole)   \n",
       "2      1489069957   2017-03-09T14:32:37Z                         Peanuts   \n",
       "3      1489055731   2017-03-09T10:35:31Z          Organic Salted Nut Mix   \n",
       "4      1489055653   2017-03-09T10:34:13Z                 Organic Polenta   \n",
       "\n",
       "  generic_name quantity         ...         ph_100g  \\\n",
       "0          NaN      1kg         ...             NaN   \n",
       "1          NaN      NaN         ...             NaN   \n",
       "2          NaN      NaN         ...             NaN   \n",
       "3          NaN      NaN         ...             NaN   \n",
       "4          NaN      NaN         ...             NaN   \n",
       "\n",
       "  fruits-vegetables-nuts_100g collagen-meat-protein-ratio_100g cocoa_100g  \\\n",
       "0                         NaN                              NaN        NaN   \n",
       "1                         NaN                              NaN        NaN   \n",
       "2                         NaN                              NaN        NaN   \n",
       "3                         NaN                              NaN        NaN   \n",
       "4                         NaN                              NaN        NaN   \n",
       "\n",
       "  chlorophyl_100g carbon-footprint_100g nutrition-score-fr_100g  \\\n",
       "0             NaN                   NaN                     NaN   \n",
       "1             NaN                   NaN                    14.0   \n",
       "2             NaN                   NaN                     0.0   \n",
       "3             NaN                   NaN                    12.0   \n",
       "4             NaN                   NaN                     NaN   \n",
       "\n",
       "  nutrition-score-uk_100g glycemic-index_100g water-hardness_100g  \n",
       "0                     NaN                 NaN                 NaN  \n",
       "1                    14.0                 NaN                 NaN  \n",
       "2                     0.0                 NaN                 NaN  \n",
       "3                    12.0                 NaN                 NaN  \n",
       "4                     NaN                 NaN                 NaN  \n",
       "\n",
       "[5 rows x 162 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Récupération du fichier en local\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "\n",
    "dataraw = pd.read_csv('C:\\Work\\OpenClassrooms\\data/fr.openfoodfacts.org.products.csv', sep=\"\\t\", low_memory=False)\n",
    "\n",
    "print(dataraw.shape)\n",
    "dataraw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On remplace les '-' des noms de colonne par '_' pour pouvoir requêter\n",
    "for col in dataraw.columns:\n",
    "    if '-' in col:\n",
    "        #print(\"rename %s in %s\" % (col, col.replace('-', '_')))\n",
    "        dataraw = dataraw.rename(columns= {col : col.replace('-', '_')})\n",
    "#dataraw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320772, 146)\n",
      "16 colonnes en moins (valeurs toutes nulles)\n",
      "45 colonnes enlevées (moins de 320 valeurs)\n",
      "(320772, 101)\n"
     ]
    }
   ],
   "source": [
    "# On enlève les colonnes dont toutes les valeurs sont nulles\n",
    "my_data = dataraw.dropna(axis=1, how='all')\n",
    "print(my_data.shape)\n",
    "print(\"%i colonnes en moins (valeurs toutes nulles)\" % (dataraw.shape[1]-my_data.shape[1]))\n",
    "\n",
    "# On enlève les colonnes avec trop peu d'information (1 pour 1000)\n",
    "nb = 0\n",
    "nbMax = (int)(my_data.shape[0] * 0.001)\n",
    "for col in dataraw.columns:\n",
    "    if my_data[col].count() < nbMax:\n",
    "        #print(\"%s : %i\" % (i, my_data[i].count()))\n",
    "        my_data = my_data.drop(col, axis=1)\n",
    "        nb += 1\n",
    "print(\"%i colonnes enlevées (moins de %i valeurs)\" % (nb, nbMax))\n",
    "print(my_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autres colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">On a encore beaucoup de colonnes dont certaines pourraient être utiles pour un client recherchant des aliments sains :\n",
    ">>- Présence d'additifs\n",
    "\n",
    ">>- Ingrédients contenant de l'huile de palme\n",
    "\n",
    ">>- Origine, pays de provenance et empreinte carbone pour une recherche de circuit court...\n",
    "\n",
    ">Ces colonnes demanderait un peu plus de temps d'analyse donc dans un premier temps on va se concentrer sur les valeurs numériques que l'on a et ne garder que les colonnes du type xxx_100g.\n",
    "\n",
    ">On va se baser sur le score français pour la suite (on verra que le score uk et le grade sont des informations redondantes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 colonnes en moins (colonnes sans valeurs nutritionnelles)\n",
      "(320772, 45)\n",
      "55387 lignes en moins (valeurs toutes nulles)\n",
      "(265385, 45)\n"
     ]
    }
   ],
   "source": [
    "# On ne garde que les colonnes avec des valeurs nutritionnelles et les scores\n",
    "my_data_nut = my_data.copy()\n",
    "for i in my_data_nut.columns:\n",
    "    if not ('_100g' in i or 'nutrition' in i):\n",
    "        #print(i)\n",
    "        my_data_nut = my_data_nut.drop(i, axis=1)\n",
    "print(\"%i colonnes en moins (colonnes sans valeurs nutritionnelles)\" % (my_data.shape[1]-my_data_nut.shape[1]))\n",
    "print(my_data_nut.shape)\n",
    "\n",
    "# On enlève les lignes dont toutes les valeurs sont nulles\n",
    "my_data_nut = my_data_nut.dropna(axis=0, how='all')\n",
    "print(\"%i lignes en moins (valeurs toutes nulles)\" % (my_data.shape[0]-my_data_nut.shape[0]))\n",
    "print(my_data_nut.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valeurs nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On met des zéros dans les valeurs nutritives nulles\n",
    "for i in my_data_nut.columns:\n",
    "    if not ('nutrition' in i):\n",
    "        my_data_nut[i] = my_data_nut[i].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valeurs négatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 valeurs négatives pour  trans_fat_100g\n",
      "7 valeurs négatives pour  sugars_100g\n",
      "1 valeurs négatives pour  fiber_100g\n",
      "3 valeurs négatives pour  proteins_100g\n",
      "1 valeurs négatives pour  vitamin_a_100g\n",
      "1 valeurs négatives pour  vitamin_c_100g\n",
      "8 valeurs négatives pour  biotin_100g\n",
      "8 valeurs négatives pour  pantothenic_acid_100g\n",
      "1 valeurs négatives pour  iron_100g\n",
      "1 valeurs négatives pour  copper_100g\n",
      "1 valeurs négatives pour  selenium_100g\n"
     ]
    }
   ],
   "source": [
    "# Annulation des valeurs nutritives négatives s'il y en a\n",
    "for i in my_data_nut.columns:\n",
    "    if 'nutrition' in i: continue\n",
    "    neg = my_data_nut[my_data_nut[i] < 0].shape[0]\n",
    "    if neg > 0:\n",
    "        print(neg, 'valeurs négatives pour ', i)\n",
    "    # On remplace par 0\n",
    "    my_data_nut[i] = my_data_nut[i].clip(lower=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265385, 45)\n",
      "Il y a 3 valeurs supérieures à 126074.000000 pour energy_100g (max=3251373.000000)\n",
      "Il y a 1 valeurs supérieures à 312.570000 pour trans_fat_100g (max=369.000000)\n",
      "Il y a 5 valeurs supérieures à 5.671000 pour cholesterol_100g (max=95.238000)\n",
      "Il y a 46 valeurs supérieures à 138.684000 pour salt_100g (max=64312.800000)\n",
      "Il y a 19 valeurs supérieures à 0.024856 pour vitamin_a_100g (max=26.700000)\n",
      "Il y a 3 valeurs supérieures à 0.000231 pour vitamin_d_100g (max=0.030000)\n",
      "Il y a 7 valeurs supérieures à 1.032000 pour vitamin_e_100g (max=15.100000)\n",
      "Il y a 2 valeurs supérieures à 0.009593 pour vitamin_k_100g (max=31.250000)\n",
      "Il y a 85 valeurs supérieures à 1.871400 pour vitamin_c_100g (max=716.981100)\n",
      "Il y a 6 valeurs supérieures à 42.929000 pour vitamin_b1_100g (max=161.000000)\n",
      "Il y a 8 valeurs supérieures à 30.357071 pour vitamin_b2_100g (max=42.500000)\n",
      "Il y a 18 valeurs supérieures à 0.598191 pour vitamin_pp_100g (max=21.428571)\n",
      "Il y a 22 valeurs supérieures à 0.124415 pour vitamin_b6_100g (max=4.000000)\n",
      "Il y a 15 valeurs supérieures à 0.004070 pour vitamin_b9_100g (max=23.076923)\n",
      "Il y a 10 valeurs supérieures à 0.016314 pour folates_100g (max=178.571429)\n",
      "Il y a 7 valeurs supérieures à 0.000388 pour vitamin_b12_100g (max=0.400000)\n",
      "Il y a 10 valeurs supérieures à 0.003441 pour biotin_100g (max=6.000000)\n",
      "Il y a 6 valeurs supérieures à 0.370867 pour pantothenic_acid_100g (max=60.000000)\n",
      "Il y a 15 valeurs supérieures à 24.150000 pour potassium_100g (max=1870.370000)\n",
      "Il y a 24 valeurs supérieures à 10.443000 pour calcium_100g (max=694.737000)\n",
      "Il y a 6 valeurs supérieures à 26.057000 pour phosphorus_100g (max=559.459000)\n",
      "Il y a 21 valeurs supérieures à 0.222090 pour iron_100g (max=47.058820)\n",
      "Il y a 10 valeurs supérieures à 12.067500 pour magnesium_100g (max=657.143000)\n",
      "Il y a 2 valeurs supérieures à 0.631500 pour zinc_100g (max=2.800000)\n",
      "Il y a 3 valeurs supérieures à 0.083200 pour copper_100g (max=16.483516)\n",
      "Il y a 6 valeurs supérieures à 0.102000 pour manganese_100g (max=0.700000)\n",
      "Il y a 2 valeurs supérieures à 0.005762 pour selenium_100g (max=3.571429)\n",
      "(265023, 45)\n"
     ]
    }
   ],
   "source": [
    "print(my_data_nut.shape)\n",
    "\n",
    "for i in my_data_nut.columns:\n",
    "    if 'nutrition' in i:\n",
    "        continue\n",
    "\n",
    "    # Etude sur un échantillon représentatif (les valeurs positives)\n",
    "    df = my_data_nut[my_data_nut[i]>0]\n",
    "#    nbTot = my_data_nut[my_data_nut[i]>0].shape[0]\n",
    "#    if nbTot < 5000:\n",
    "#        print(\"Trop peu de valeurs (%i) pour %s\" % (nbTot, i))\n",
    "#        continue\n",
    "\n",
    "    # Outliers\n",
    "    q75, q25 = np.percentile(df[i], [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    #min = q25 - (iqr*1.5)\n",
    "    #my_data_nut = my_data_nut[my_data_nut[i]>min]\n",
    "\n",
    "    max = q75 + (iqr*100) # Je prends une valeur très haute car il sinon on sort trop de lignes\n",
    "    nbOutliers = my_data_nut[my_data_nut[i]>max].shape[0]\n",
    "    if nbOutliers > 0:\n",
    "        print(\"Il y a %i valeurs supérieures à %f pour %s (max=%f)\" % (nbOutliers, max, i, np.max(my_data_nut[i])))\n",
    "\n",
    "    my_data_nut = my_data_nut[my_data_nut[i]<=max]\n",
    "    \n",
    "print(my_data_nut.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On exporte un fichier light pour une étude rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221017, 45)\n",
      "(30000, 45)\n"
     ]
    }
   ],
   "source": [
    "col_score = ['nutrition_score_fr_100g', 'nutrition_score_uk_100g', 'nutrition_grade_fr']\n",
    "\n",
    "# On ne garde que les scores non nulls\n",
    "dflight = my_data_nut.copy()\n",
    "dflight.dropna(subset=col_score, inplace=True)\n",
    "print(dflight.shape)\n",
    "dflight.to_csv('clean_products_light.csv', sep=\"\\t\")\n",
    "\n",
    "# On travaille aussi sur un échantillon pour la mémoire et le temps de calcul\n",
    "dflight = dflight.sample(30000)\n",
    "print(dflight.shape)\n",
    "dflight.to_csv('clean_products_light_sample.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recherche des scores manquants par regression knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics, preprocessing, neighbors, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend un échantillon pour des tests plus rapide\n",
    "my_data_nut = my_data_nut.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 1679 valeurs nulles\n",
      "Il y a 8321 valeurs non nulles\n"
     ]
    }
   ],
   "source": [
    "# Séparation des valeurs nulles et non nulles\n",
    "dfScoreNull = my_data_nut[pd.isnull(my_data_nut.nutrition_score_fr_100g)]\n",
    "print('Il y a', dfScoreNull.shape[0], 'valeurs nulles')\n",
    "dfScoreNotNull = my_data_nut[pd.notnull(my_data_nut.nutrition_score_fr_100g)]\n",
    "print('Il y a', dfScoreNotNull.shape[0], 'valeurs non nulles')\n",
    "\n",
    "# On sépare les colonnes de scores sur les valeurs non nulles\n",
    "X = dfScoreNotNull.drop(col_score, axis=1)\n",
    "y = dfScoreNotNull.nutrition_score_fr_100g\n",
    "\n",
    "# 30% des données dans le jeu de test\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Standardisation\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMSE = 12.417 (+/-3.011) for {'n_neighbors': 3}\n",
      "\tMSE = 11.953 (+/-2.361) for {'n_neighbors': 5}\n",
      "\tMSE = 12.157 (+/-2.471) for {'n_neighbors': 7}\n",
      "\tMSE = 12.374 (+/-2.430) for {'n_neighbors': 9}\n",
      "\tMSE = 12.514 (+/-2.200) for {'n_neighbors': 11}\n",
      "\tMSE = 12.735 (+/-2.114) for {'n_neighbors': 13}\n",
      "\tMSE = 12.944 (+/-2.070) for {'n_neighbors': 15}\n",
      "Meilleur k : 5\n"
     ]
    }
   ],
   "source": [
    "# On fait un grid search pour trouver le meilleur k\n",
    "param_grid = {'n_neighbors':[3, 5, 7, 9, 11, 13, 15]}\n",
    "mse = 'neg_mean_squared_error'\n",
    "gs = model_selection.GridSearchCV(neighbors.KNeighborsRegressor(), param_grid, scoring=mse, cv = 5)\n",
    "\n",
    "gs.fit(X_train_std, y_train)\n",
    "me = []\n",
    "err = []\n",
    "for mean, std, params in zip(gs.cv_results_['mean_test_score'], # score moyen\n",
    "                             gs.cv_results_['std_test_score'], # écart-type du score\n",
    "                             gs.cv_results_['params'] # valeur de l'hyperparamètre\n",
    "                            ):\n",
    "    print (\"\\t%s = %0.3f (+/-%0.03f) for %r\" % ('MSE', -mean, std * 2, params))\n",
    "    me.append(mean)\n",
    "    err.append(2*std)\n",
    "\n",
    "kBest = gs.best_params_['n_neighbors']\n",
    "print ('Meilleur k : %r' % kBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur sur le jeu de test : MSE = 11.43\n"
     ]
    }
   ],
   "source": [
    "# Entrainement du meilleur modèle et score sur le jeu de test\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=kBest)\n",
    "knn.fit(X_train_std, y_train)\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print (\"Erreur sur le jeu de test : MSE = %.2f\" % metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode naive par la moyenne : MSE = 81.80\n"
     ]
    }
   ],
   "source": [
    "# Méthode naive par la moyenne pour comparer\n",
    "from sklearn import dummy\n",
    "dum = dummy.DummyRegressor(strategy='mean')\n",
    "# Entraînement\n",
    "dum.fit(X_train_std, y_train)\n",
    "# Prédiction sur le jeu de test\n",
    "y_pred_dum = dum.predict(X_test_std)\n",
    "# Evaluate\n",
    "print (\"Méthode naive par la moyenne : MSE = %.2f\" % metrics.mean_squared_error(y_test, y_pred_dum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle est bien meilleur que le modèle naif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 45)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recherche des scores nulls\n",
    "X = dfScoreNull.drop(col_score, axis=1)\n",
    "\n",
    "# Standardisation\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = std_scale.transform(X)\n",
    "\n",
    "yScoreNull = knn.predict(X_scaled)\n",
    "\n",
    "# Remplacement des nulls\n",
    "dfScoreNull.loc[:,'nutrition_score_fr_100g'] = yScoreNull\n",
    "\n",
    "my_data_nut = dfScoreNotNull.append(dfScoreNull)\n",
    "my_data_nut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recherche des grades manquants par classification knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 1679 valeurs nulles\n",
      "Il y a 8321 valeurs non nulles\n"
     ]
    }
   ],
   "source": [
    "# Séparation des valeurs nulles et non nulles\n",
    "dfScoreNull = my_data_nut[pd.isnull(my_data_nut.nutrition_grade_fr)]\n",
    "print('Il y a', dfScoreNull.shape[0], 'valeurs nulles')\n",
    "dfScoreNotNull = my_data_nut[pd.notnull(my_data_nut.nutrition_grade_fr)]\n",
    "print('Il y a', dfScoreNotNull.shape[0], 'valeurs non nulles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On sépare les colonnes de scores sur les valeurs non nulles\n",
    "X = dfScoreNotNull.drop(col_score, axis=1)\n",
    "y = dfScoreNotNull.nutrition_grade_fr\n",
    "\n",
    "# Transformation des grades en valeurs de 1 à 5, correspondant à (['a', 'b', 'c', 'd', 'e'])\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "y = label_encoder.transform(y)\n",
    "label_encoder.classes_\n",
    "\n",
    "# 30% des données dans le jeu de test\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Standardisation\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMSE = 0.623 (+/-0.128) for {'n_neighbors': 3}\n",
      "\tMSE = 0.598 (+/-0.089) for {'n_neighbors': 5}\n",
      "\tMSE = 0.623 (+/-0.100) for {'n_neighbors': 7}\n",
      "\tMSE = 0.619 (+/-0.079) for {'n_neighbors': 9}\n",
      "\tMSE = 0.649 (+/-0.068) for {'n_neighbors': 11}\n",
      "\tMSE = 0.655 (+/-0.050) for {'n_neighbors': 13}\n",
      "\tMSE = 0.660 (+/-0.040) for {'n_neighbors': 15}\n",
      "Meilleur k : 5\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors':[3, 5, 7, 9, 11, 13, 15]}\n",
    "mse = 'neg_mean_squared_error'\n",
    "gs = model_selection.GridSearchCV(neighbors.KNeighborsClassifier(), param_grid, scoring=mse, cv = 5)                                 \n",
    "\n",
    "gs.fit(X_train_std, y_train)\n",
    "me = []\n",
    "err = []\n",
    "for mean, std, params in zip(gs.cv_results_['mean_test_score'], # score moyen\n",
    "                             gs.cv_results_['std_test_score'], # écart-type du score\n",
    "                             gs.cv_results_['params'] # valeur de l'hyperparamètre\n",
    "                            ):\n",
    "    print (\"\\t%s = %0.3f (+/-%0.03f) for %r\" % ('MSE', -mean, std * 2, params))\n",
    "    me.append(mean)\n",
    "    err.append(2*std)\n",
    "kBest = gs.best_params_['n_neighbors']\n",
    "print ('Meilleur k : %r' % kBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur sur le jeu de test : MSE = 0.48\n"
     ]
    }
   ],
   "source": [
    "# Entrainement du meilleur modèle\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=kBest)\n",
    "knn.fit(X_train_std, y_train)\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print (\"Erreur sur le jeu de test : MSE = %.2f\" % metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Méthode naive par la moyenne : MSE = 2.54\n"
     ]
    }
   ],
   "source": [
    "# Méthode naive par la classe la plus fréquente pour comparer\n",
    "dum = dummy.DummyClassifier(strategy='most_frequent')\n",
    "# Entraînement\n",
    "dum.fit(X_train_std, y_train)\n",
    "# Prédiction sur le jeu de test\n",
    "y_pred_dum = dum.predict(X_test_std)\n",
    "# Evaluate\n",
    "print (\"Méthode naive : MSE = %.2f\" % metrics.mean_squared_error(y_test, y_pred_dum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle est bien meilleur que le modèle naif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 45)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recherche des scores nulls\n",
    "X = dfScoreNull.drop(col_score, axis=1)\n",
    "\n",
    "# Standardisation\n",
    "std_scale = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = std_scale.transform(X)\n",
    "\n",
    "yScoreNull = knn.predict(X_scaled)\n",
    "\n",
    "#On remet les grades\n",
    "yScoreNull = label_encoder.inverse_transform(yScoreNull)\n",
    "\n",
    "# Remplacement des nulls\n",
    "dfScoreNull.loc[:,'nutrition_grade_fr'] = yScoreNull\n",
    "\n",
    "my_data_nut = dfScoreNotNull.append(dfScoreNull)\n",
    "my_data_nut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data_nut.to_csv('clean_products_full.csv', sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
